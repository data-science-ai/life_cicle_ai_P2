# ğŸš€ Chat con IA Local

Una aplicaciÃ³n de escritorio desarrollada en Python que permite interactuar con modelos de IA locales a travÃ©s de una interfaz grÃ¡fica intuitiva, compatible con Ollama en Docker.

## âœ¨ CaracterÃ­sticas Principales

- ğŸ–¥ï¸ Interfaz grÃ¡fica intuitiva desarrollada con Tkinter
- ğŸ¤– Soporte para mÃºltiples modelos de IA locales a travÃ©s de Ollama
- âš™ï¸ ConfiguraciÃ³n personalizable mediante variables de entorno
- ğŸ’¬ Intercambio de mensajes en tiempo real
- ğŸ³ FÃ¡cil despliegue con Docker
- ğŸ”„ Historial de conversaciÃ³n
- ğŸ¨ Tema oscuro/claro

## ğŸ“‹ Requisitos del Sistema

### MÃ­nimos
- Python 3.8 o superior
- Docker y Docker Compose
- 8GB de RAM (mÃ­nimo)
- 10GB de espacio en disco

### Recomendados
- 16GB+ de RAM
- GPU compatible con CUDA para mejor rendimiento
- ConexiÃ³n a Internet para descargar modelos

## ğŸš€ Comenzando RÃ¡pidamente

### Con Docker (Recomendado)

1. **Clonar el repositorio**
   ```bash
   git clone https://github.com/tu-usuario/ai_cicle_2.git
   cd ai_cicle_2
   ```

2. **Configurar variables de entorno**
   Copia el archivo de plantilla y edÃ­talo segÃºn tus necesidades:
   ```bash
   cp .env.template .env
   ```

   ConfiguraciÃ³n bÃ¡sica del archivo `.env`:
   ```env
   # ConfiguraciÃ³n de Ollama
   BASE_URL=http://localhost:11434/api/generate
   
   # Modelo predeterminado (puedes cambiarlo por cualquier modelo soportado por Ollama)
   MODEL=llama3
   
   # MÃ¡ximo de tokens por respuesta
   MAX_TOKENS=1000
   
   # Opcional: ConfiguraciÃ³n de proxy si es necesario
   # HTTP_PROXY=
   # HTTPS_PROXY=
   ```

3. **Iniciar los servicios**
   ```bash
   docker-compose up -d
   ```

4. **Descargar un modelo**
   ```bash
   # Descargar un modelo (ejemplo con llama3)
   docker exec ollama ollama pull llama3
   
   # Listar modelos disponibles
   # docker exec ollama ollama list
   ```

5. **Iniciar la aplicaciÃ³n**
   ```bash
   python app.py
   ```

### InstalaciÃ³n Manual

1. **Clonar el repositorio**
   ```bash
   git clone https://github.com/tu-usuario/ai_cicle_2.git
   cd ai_cicle_2
   ```

2. **Crear y activar un entorno virtual (recomendado)**
   ```bash
   python -m venv venv
   source venv/bin/activate  # En Windows: .\venv\Scripts\activate
   ```

3. **Instalar dependencias**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configurar variables de entorno**
   Sigue los mismos pasos que en la secciÃ³n de Docker para configurar el archivo `.env`.

5. **Iniciar la aplicaciÃ³n**
   ```bash
   python app.py
   ```

## ğŸ› ï¸ ConfiguraciÃ³n de Ollama

Si no estÃ¡s usando Docker, necesitarÃ¡s instalar y ejecutar Ollama manualmente:

```bash
# En Linux/macOS
curl -fsSL https://ollama.com/install.sh | sh
ollama serve &

# O con Docker (si prefieres no usar el docker-compose proporcionado)
docker run -d -p 11434:11434 --name ollama ollama/ollama
```

## ğŸ“± Uso de la AplicaciÃ³n

1. **Iniciar la aplicaciÃ³n**
   ```bash
   python app.py
   ```

2. **Interfaz de usuario**
   - Escribe tu pregunta en el campo de texto inferior
   - Presiona Enter o haz clic en el botÃ³n "Enviar"
   - Las respuestas aparecerÃ¡n en el Ã¡rea de chat principal
   - Usa el botÃ³n "Limpiar chat" para comenzar una nueva conversaciÃ³n

3. **CaracterÃ­sticas adicionales**
   - Cambia entre tema claro/oscuro usando el botÃ³n en la esquina superior derecha
   - Las conversaciones se guardan automÃ¡ticamente
   - Soporte para formato Markdown en las respuestas



## ğŸ”§ ConfiguraciÃ³n Avanzada

### Variables de Entorno
Crea un archivo `.env` en la raÃ­z del proyecto con:
```env
BASE_URL=http://localhost:11434/api/generate
MODEL=llama3  # Modelo a utilizar
MAX_TOKENS=1000  # MÃ¡ximo de tokens por respuesta
```

### Modelos Disponibles
Puedes usar cualquier modelo compatible con Ollama. Algunos populares:
- `llama3`
- `mistral`
- `llama2`
- `dolphin-mistral`

Para ver los modelos instalados:
```bash
docker exec ollama ollama list
```

## ğŸ—ï¸ Estructura del Proyecto

```
.
â”œâ”€â”€ app.py              # AplicaciÃ³n principal
â”œâ”€â”€ requirements.txt    # Dependencias de Python
â”œâ”€â”€ docker-compose.yml  # ConfiguraciÃ³n de Docker
â”œâ”€â”€ .env.example       # Plantilla de configuraciÃ³n
â”œâ”€â”€ README.md          # Este archivo
â””â”€â”€ .gitignore         # Archivos ignorados por Git
```

## ğŸ” SoluciÃ³n de Problemas

- **Error de conexiÃ³n**: Verifica que Ollama estÃ© en ejecuciÃ³n y accesible en `http://localhost:11434`
- **Modelo no encontrado**: AsegÃºrate de haber descargado el modelo con `ollama pull <nombre-modelo>`
- **Problemas de memoria**: Algunos modelos requieren mucha RAM. Prueba con un modelo mÃ¡s pequeÃ±o si tienes problemas.

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la [Licencia MIT](LICENSE).
