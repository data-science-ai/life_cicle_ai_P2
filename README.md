# ğŸš€ Chat con IA Local

Una aplicaciÃ³n de escritorio desarrollada en Python que permite interactuar con modelos de IA locales a travÃ©s de una interfaz grÃ¡fica sencilla, compatible con Ollama en Docker.

## âœ¨ CaracterÃ­sticas

- Interfaz grÃ¡fica intuitiva con Tkinter
- ConexiÃ³n con modelos de IA locales a travÃ©s de Ollama
- ConfiguraciÃ³n mediante variables de entorno
- Intercambio de mensajes en tiempo real
- FÃ¡cil despliegue con Docker

## ğŸ“‹ Requisitos Previos

- Python 3.8 o superior
- Docker y Docker Compose
- Al menos 8GB de RAM recomendados para ejecutar los modelos

## ğŸ³ ConfiguraciÃ³n con Docker (Recomendado)

1. Clona este repositorio:
   ```bash
   git clone https://github.com/tu-usuario/ai_cicle_2.git
   cd ai_cicle_2
   ```

2. Crea un archivo `.env` con la configuraciÃ³n:
   ```env
   BASE_URL=http://localhost:11434/api/generate
   MODEL=llama3
   MAX_TOKENS=1000
   ```

3. Inicia los servicios con Docker Compose:
   ```bash
   docker-compose up -d
   ```

4. Descarga un modelo (ejemplo con llama3):
   ```bash
   docker exec ollama ollama pull llama3
   ```

## ğŸ’» InstalaciÃ³n Manual

1. Instala las dependencias:
   ```bash
   pip install -r requirements.txt
   ```

2. AsegÃºrate de tener Ollama en ejecuciÃ³n:
   ```bash
   # En Linux/macOS
   curl -fsSL https://ollama.com/install.sh | sh
   ollama serve &
   
   # O con Docker
   docker run -d -p 11434:11434 --name ollama ollama/ollama
   ```

## ğŸš€ Uso

1. Inicia la aplicaciÃ³n:
   ```bash
   python app.py
   ```

2. Interfaz de usuario:
   - Escribe tu pregunta en el campo de texto
   - Presiona Enter o haz clic en "Enviar"
   - La respuesta aparecerÃ¡ en el Ã¡rea de chat

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Variables de Entorno
Crea un archivo `.env` en la raÃ­z del proyecto con:
```env
BASE_URL=http://localhost:11434/api/generate
MODEL=llama3  # Modelo a utilizar
MAX_TOKENS=1000  # MÃ¡ximo de tokens por respuesta
```

### Modelos Disponibles
Puedes usar cualquier modelo compatible con Ollama. Algunos populares:
- `llama3`
- `mistral`
- `llama2`
- `dolphin-mistral`

Para ver los modelos instalados:
```bash
docker exec ollama ollama list
```

## ğŸ—ï¸ Estructura del Proyecto

```
.
â”œâ”€â”€ app.py              # AplicaciÃ³n principal
â”œâ”€â”€ requirements.txt    # Dependencias de Python
â”œâ”€â”€ docker-compose.yml  # ConfiguraciÃ³n de Docker
â”œâ”€â”€ .env.example       # Plantilla de configuraciÃ³n
â”œâ”€â”€ README.md          # Este archivo
â””â”€â”€ .gitignore         # Archivos ignorados por Git
```

## ğŸ” SoluciÃ³n de Problemas

- **Error de conexiÃ³n**: Verifica que Ollama estÃ© en ejecuciÃ³n y accesible en `http://localhost:11434`
- **Modelo no encontrado**: AsegÃºrate de haber descargado el modelo con `ollama pull <nombre-modelo>`
- **Problemas de memoria**: Algunos modelos requieren mucha RAM. Prueba con un modelo mÃ¡s pequeÃ±o si tienes problemas.

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la [Licencia MIT](LICENSE).
